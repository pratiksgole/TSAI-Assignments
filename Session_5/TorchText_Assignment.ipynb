{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TorchText_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGxpagGh64vi"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.datasets import YelpReviewFull, YelpReviewPolarity, AmazonReviewPolarity"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUsqHehK8CBD"
      },
      "source": [
        "class TextClassification(nn.Module):\n",
        "      def __init__(self, vocab_size, embedding_dim, num_classes, hidden_dim):\n",
        "          super(TextClassification, self).__init__()\n",
        "          self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim)          \n",
        "          self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "          self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "          self.init_weight()\n",
        "      def init_weight(self):\n",
        "          initrange = 0.5\n",
        "          self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "          self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "          self.fc1.bias.data.zero_()\n",
        "          self.fc2.weight.data.uniform_(-initrange, initrange)\n",
        "          self.fc2.bias.data.zero_()\n",
        "\n",
        "      def forward(self, text, offset):\n",
        "          out = self.embedding(text, offset)\n",
        "          out = torch.relu(self.fc1(out))\n",
        "          return self.fc2(out)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4QHEgYYkcYI"
      },
      "source": [
        "def get_token(data_iter, tokenizer):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "def collate_batch(batch):\n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  for (_label, _text) in batch:\n",
        "      label_list.append(label_pipeline(_label))\n",
        "      processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "      text_list.append(processed_text)\n",
        "      offsets.append(processed_text.size(0))\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
        "\n",
        "\n",
        "def train(dataloader, model, optimizer):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d}/{:5d} batches | accuracy {:8.3f}'.format(idx, len(dataloader), total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    df = {}\n",
        "    pred, org = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "\n",
        "            pred.extend(predited_label.argmax(1))\n",
        "            org.extend(label)\n",
        "            total_count += label.size(0)\n",
        "    df = pd.DataFrame({'pred': pred, 'org': org})\n",
        "    return total_acc/total_count, df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKb6qEd8FFHw"
      },
      "source": [
        "def main(ds_name):\n",
        "      model = TextClassification(vocab_size, embedding_dim, num_classes, hidden_dim)\n",
        "\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "      total_accu = None\n",
        "      train_iter, test_iter = ds_name()\n",
        "      train_dataset = to_map_style_dataset(train_iter)\n",
        "      test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "\n",
        "\n",
        "      num_train = int(len(train_dataset) * 0.95)\n",
        "      split_train_, split_valid_ = \\\n",
        "          random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "      train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                                    shuffle=True, collate_fn=collate_batch)\n",
        "      valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                                    shuffle=True, collate_fn=collate_batch)\n",
        "      test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "      for epoch in range(1, EPOCHS + 1):\n",
        "          epoch_start_time = time.time()\n",
        "          train(train_dataloader, model, optimizer)\n",
        "          accu_val, df = evaluate(valid_dataloader, model)\n",
        "          if total_accu is not None and total_accu > accu_val:\n",
        "            scheduler.step()\n",
        "          else:\n",
        "            total_accu = accu_val\n",
        "          print('-' * 59)\n",
        "          print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, accu_val))\n",
        "          print('-' * 59)\n",
        "      return df\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqPcblCze1m4"
      },
      "source": [
        "def classwise_prediction_accuracy(df):\n",
        "    df['pred'] = df['pred'].apply(lambda x: int(x))\n",
        "    df['org'] = df['org'].apply(lambda x: int(x))\n",
        "    df['Match'] = df.apply(lambda x: 1 if x['pred']==x['org'] else 0, axis=1)\n",
        "    df.groupby('org')['Match'].sum()\n",
        "    return df.groupby('org')['Match'].sum()/df['org'].value_counts().reset_index().sort_values(by='index')['org']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__pdD9DkXxs8"
      },
      "source": [
        "dataset_list = [YelpReviewPolarity, AmazonReviewPolarity]\n",
        "\n",
        "for ds_name in dataset_list:\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Hyperparameters\n",
        "    EPOCHS =  20 # epoch\n",
        "    LR = 5  # learning rate\n",
        "    BATCH_SIZE = 96 # batch size for training\n",
        "\n",
        "    train_iter = ds_name(split='train')\n",
        "    vocab_out = build_vocab_from_iterator(get_token(train_iter, tokenizer), specials=[\"<unk>\"],  min_freq= 5)\n",
        "    vocab_out.set_default_index(vocab_out['<unk>'])\n",
        "\n",
        "    text_pipeline = lambda x: vocab_out(tokenizer(x))\n",
        "    label_pipeline = lambda x: x -1\n",
        "\n",
        "    train_iter = ds_name(split='train')\n",
        "    vocab_size = len(vocab_out)\n",
        "    embedding_dim = 64\n",
        "    hidden_dim = 32\n",
        "    num_classes = len(set([label for label, _ in train_iter]))\n",
        "    df_out = main(ds_name)\n",
        "    print(f'Classwiswe Prediction for {ds_name} dataset: {classwise_prediction_accuracy(df_out)}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKDOJSrKhwbR"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}